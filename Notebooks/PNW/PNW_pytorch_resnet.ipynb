{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "#Get rid of SettingWithCopyWarnings\n",
    "pd.options.mode.chained_assignment = None  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PNW = pd.read_csv('../../Data/processedData/PNW/PNW.csv')\n",
    "PNW = PNW.dropna()\n",
    "PNW[\"flowering\"] = PNW['reproductiveCondition'].isin(['flowering','flowering|fruiting','flowering|fruiting|flower budding','flowering|flower budding']).astype(int)\n",
    "PNW[\"fruiting\"] = PNW['reproductiveCondition'].isin(['fruiting','flowering|fruiting','flowering|fruiting|flower budding','fruiting|flower budding']).astype(int)\n",
    "\n",
    "photoPath = \"../../../PNW_Angiosperms/labeled_photos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I only have 49,999 so I'm missing one photo. The following code drops that row from the dataframe\n",
    "for fname in PNW['file_name']: \n",
    "    if not os.path.isfile(photoPath + \"/\" + fname):\n",
    "        PNW = PNW[PNW['file_name'] != fname]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "photos = PNW[[\"file_name\", \"flowering\"]].reset_index(drop=True)\n",
    "photos_train, photos_val = train_test_split(list(photos.index), test_size=.25)\n",
    "p_train= photos.iloc[photos_train]\n",
    "p_val = photos.iloc[photos_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define relevant variables for the ML task\n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "\n",
    "# Device will determine whether to run the training on GPU or CPU.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sean Haight\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Some of our images are .pngs, we drop the alpha. \n",
    "train_transform = v2.Compose([\n",
    "    #Rely on the v2 transforms from torchvision.transforms.v2\n",
    "\n",
    "    #Use tensors instead of PIL images\n",
    "\n",
    "    #Use torch.uint8 dtype, especially for resizing\n",
    "                                v2.ToPILImage(),\n",
    "                                v2.ToTensor(),\n",
    "                                v2.RandomAffine(degrees=(-180,180), translate=(0,.1), scale=(.9,1)),\n",
    "                                v2.RandomHorizontalFlip(p=0.5),\n",
    "                                v2.RandomVerticalFlip(p=0.5),\n",
    "                                v2.ColorJitter(brightness=.3, hue=.01),\n",
    "                                v2.Resize ( (256,256) , interpolation=2 ),\n",
    "                                v2.CenterCrop(224),\n",
    "                                v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                               #transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "\n",
    "                            ])\n",
    "val_transform = v2.Compose([\n",
    "                                #transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "                                v2.ToPILImage(),\n",
    "                                v2.ToTensor(),\n",
    "                                torchvision.transforms.Resize ( (224,224) , interpolation=2 ),\n",
    "                                v2.Resize ( (256,256) , interpolation=2 ),\n",
    "                                v2.CenterCrop(224),\n",
    "                                v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                            ])\n",
    "target_transform = v2.Compose([\n",
    "                                v2.Lambda(lambda x: torch.tensor(x).long()),\n",
    "                            ])                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, label_frame, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = label_frame\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CustomImageDataset(p_train, photoPath, train_transform, target_transform)\n",
    "val_data = CustomImageDataset(p_val, photoPath, val_transform, target_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, num_epochs, lr):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        \n",
    "        #Turn the model into training mode\n",
    "        model.train()\n",
    "        for data, target in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            target=target.to(device)   \n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * data.size(0)\n",
    "            _, pred = torch.max(output, 1)\n",
    "            train_correct += (pred == target).sum().item()\n",
    "\n",
    "        train_loss /= len(train_dataloader.dataset)\n",
    "        train_acc = 100.0 * train_correct / len(train_dataloader.dataset)\n",
    "\n",
    "        test_loss = 0.0\n",
    "        test_correct = 0\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_dataloader:\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "\n",
    "                test_loss += loss.item() * data.size(0)\n",
    "                _, pred = torch.max(output, 1)\n",
    "                test_correct += (pred == target).sum().item()\n",
    "\n",
    "        test_loss /= len(val_dataloader.dataset)\n",
    "        test_acc = 100.0 * test_correct / len(val_dataloader.dataset)\n",
    "\n",
    "        print(f'Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Sean Haight/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "c:\\Users\\Sean Haight\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sean Haight\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 10\u001b[0m\n\u001b[0;32m      5\u001b[0m model18\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m      6\u001b[0m     nn\u001b[38;5;241m.\u001b[39mDropout(\u001b[38;5;241m0.5\u001b[39m),\n\u001b[0;32m      7\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(num_ftrs, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m      9\u001b[0m model18\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 10\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel18\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.001\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 18\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, num_epochs, lr)\u001b[0m\n\u001b[0;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 18\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m data\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     19\u001b[0m _, pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(output, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     20\u001b[0m train_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (pred \u001b[38;5;241m==\u001b[39m target)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model18 = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.SGD(model18.parameters(), lr=0.001, momentum=0.9)\n",
    "num_ftrs = model18.fc.in_features\n",
    "model18.fc = nn.Sequential(\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(num_ftrs, 2)\n",
    ")\n",
    "model18.to(device)\n",
    "train(model18, 40, .001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#According to Bum Jun Kim I want to add dropout after the last batch normalization but before the last weight layer in the residual branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sean Haight\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.6814, Train Acc: 67.58%, Test Loss: 0.5429, Test Acc: 70.67%\n",
      "Epoch 2: Train Loss: 0.5301, Train Acc: 71.49%, Test Loss: 0.5153, Test Acc: 72.27%\n",
      "Epoch 3: Train Loss: 0.5130, Train Acc: 72.19%, Test Loss: 0.4862, Test Acc: 74.54%\n",
      "Epoch 4: Train Loss: 0.4990, Train Acc: 73.07%, Test Loss: 0.4802, Test Acc: 73.92%\n",
      "Epoch 5: Train Loss: 0.4893, Train Acc: 73.95%, Test Loss: 0.4742, Test Acc: 74.60%\n",
      "Epoch 6: Train Loss: 0.4849, Train Acc: 74.38%, Test Loss: 0.4655, Test Acc: 75.70%\n",
      "Epoch 7: Train Loss: 0.4780, Train Acc: 74.94%, Test Loss: 0.4613, Test Acc: 76.14%\n",
      "Epoch 8: Train Loss: 0.4729, Train Acc: 75.18%, Test Loss: 0.4617, Test Acc: 75.69%\n",
      "Epoch 9: Train Loss: 0.4651, Train Acc: 75.64%, Test Loss: 0.4461, Test Acc: 76.98%\n",
      "Epoch 10: Train Loss: 0.4648, Train Acc: 75.60%, Test Loss: 0.4425, Test Acc: 77.02%\n",
      "Epoch 11: Train Loss: 0.4617, Train Acc: 75.62%, Test Loss: 0.4575, Test Acc: 75.80%\n",
      "Epoch 12: Train Loss: 0.4559, Train Acc: 76.22%, Test Loss: 0.4379, Test Acc: 77.74%\n",
      "Epoch 13: Train Loss: 0.4542, Train Acc: 76.62%, Test Loss: 0.4471, Test Acc: 76.23%\n",
      "Epoch 14: Train Loss: 0.4500, Train Acc: 76.68%, Test Loss: 0.4380, Test Acc: 77.50%\n",
      "Epoch 15: Train Loss: 0.4492, Train Acc: 76.59%, Test Loss: 0.4345, Test Acc: 78.01%\n",
      "Epoch 16: Train Loss: 0.4476, Train Acc: 76.93%, Test Loss: 0.4327, Test Acc: 77.78%\n",
      "Epoch 17: Train Loss: 0.4439, Train Acc: 77.24%, Test Loss: 0.4307, Test Acc: 77.78%\n",
      "Epoch 18: Train Loss: 0.4412, Train Acc: 77.38%, Test Loss: 0.4290, Test Acc: 78.10%\n",
      "Epoch 19: Train Loss: 0.4392, Train Acc: 77.54%, Test Loss: 0.4331, Test Acc: 77.49%\n",
      "Epoch 20: Train Loss: 0.4393, Train Acc: 77.48%, Test Loss: 0.4446, Test Acc: 76.91%\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "from torchvision.models.resnet import ResNet, BasicBlock, Bottleneck\n",
    "\n",
    "class SeanBlock(BasicBlock):\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1, base_width=64, dilation=1, norm_layer=None):\n",
    "        super(SeanBlock, self).__init__(inplanes, planes, stride, downsample, groups, base_width, dilation, norm_layer)\n",
    "        self.dropout_prob = .5\n",
    "        \n",
    "    def forward(self,x):\n",
    "        identity = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = nn.Dropout(self.dropout_prob)(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "    \n",
    "class SeanBottleneck(Bottleneck):\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1, base_width=64, dilation=1, norm_layer=None):\n",
    "        super(SeanBottleneck, self).__init__(inplanes, planes, stride, downsample, groups, base_width, dilation, norm_layer)\n",
    "        self.dropout_prob = .5\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = nn.Dropout(self.dropout_prob)(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = nn.Dropout(self.dropout_prob)(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "class MyResNet50(ResNet):\n",
    "    def __init__(self):\n",
    "        super(MyResNet50, self).__init__(SeanBottleneck, [3, 4, 6, 3])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # change forward here\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = MyResNet50()\n",
    "# if you need pretrained weights\n",
    "model.load_state_dict(models.resnet50(weights=True).state_dict())\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "model.to(device)\n",
    "train(model, 20, .001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
